#### 8.1

n步自举法的更新开始时比Dyna更有效率, 随机抽样已观察到的状态价值很消耗计算资源因为许多状态还没有收到反向传播的reward.但是当幕数变多时, 这种消耗会减少. 随着训练继续, n步自举法的限制在于其需要与环境的直接交互, 而Dyna有模型学习的功能.

#### 8.2

Dyna-Q+有更有效的试探机制. Dyna-Q只依赖于 $\epsilon$-greedy, 更难找到最优路径.

#### 8.3

因为Dyna-Q+在找到最优路径后依然会出现试探. 而Dyna-Q不会.

