# RL note

## 第二章

### 2.1 k臂赌博机

$k$臂赌博机问题:重复地在$k$个选项中进行选择，做出选择后可得到一定收益。目标是在某一段时间内最大化总收益的期望。

**价值**：动作的期望或平均收益。

$A_t$ : 时刻$t$选择的动作

$R_t$ : $A_t$对应的收益

$q_*(a)$ : 动作$a$的价值(收益的期望)

$Q_t(a)$ : 动作$a$在$t$时刻的价值估计

我们希望$Q_t(a)$能够接近$q_*(a)$.

**贪心**:最高估计价值的动作

开发:选择贪心的动作

试探:选择非贪心的动作



### 2.2 动作-价值方法

**动作-价值方法**:使用价值估计来选择动作

一种方式是计算实际收益的平均值$Q_t(a)$:
$$
Q_t(a)\doteq \frac{t时刻前执行动作a得到的收益总和}{t时刻前执行动作a的次数}=\frac{\sum_{i=1}^{t-1}R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}}
$$
分母为0时,可以将$Q_t(a)$定义为某个默认值,比如0.

分母趋向无穷大时,$Q_t(a)$会收敛到$q_*(a)$,这称为**采样平均方法**.

**贪心**的选择方式可以记作:
$$
A_t\doteq \underset {a}{\arg \max}Q_t(a)
$$
一种替代策略:$\epsilon$-贪心($\epsilon$-greedy):

​		以很小的概率$\epsilon$从所有动作中等概率选择,大部分时间贪心.

其优点是时刻无限长时,每一个动作会被无限次采样,确保$Q_t(a)$会收敛到$q_*(a)$.

### 2.4 增量式实现

为简化标记,仅关心单个动作.

$R_i$:这一动作被选择$i$次的后获得的收益.

$Q_n$:被选择$n-1$次后的估计价值
$$
Q_n \doteq \frac{R_1+R_2+\cdots + R_{n-1}}{n-1}
$$
这种实现需要记录所有收益的记录,显然不适合.

**增量式公式**:

给定$Q_n$和第$n$次收益$R_n$的情况下,
$$
\begin{aligned}
Q_{n+1}&=\frac{1}{n}\sum_{i=1}^{n}R_i\\
&=\frac{1}{n}(R_n+\sum_{i=1}^{n-1}R_i)\\
&=\frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i)\\
&=\frac{1}{n}(R_n+(n-1)Q_n)\\
&=\frac{1}{n}(R_n+nQ_n-Q_n)\\
&=Q_n+\frac{1}{n}[R_n-Q_n]
\end{aligned}
$$
对于每一个新的收益,这种形式只需存储$Q_n$和$n$,计算量少.

对此我们得到一个公式的一般形式,即
$$
新估计值\leftarrow旧估计值+步长\times [目标-旧估计值]
$$
[目标-旧估计值]是估计值的**误差**.

$\alpha$或$\alpha_t(a)$表示**步长**,这是一个变量.

### 2.5 非平稳问题

 **非平稳问题**即收益概率是随时间变化的问题.

对于这种问题,我们应该赋予**近期**的收益**更高的权值**.

对于增量更新公式
$$
Q_{n+1}=Q_n+\alpha[R_n-Q_n]
$$
可以将其写成
$$
\begin{aligned}
Q_{n+1}&=\alpha R_n+(1-\alpha)Q_n\\
&=\alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}]\\
&=\alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2Q_{n-1}\\
&=\alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2R_{n-2}+\cdots +(1-\alpha)^{n-1}\alpha R_1+(1-\alpha)^nQ_1\\
&=(1-\alpha)^nQ_1+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}R_i
\end{aligned}
$$
这称为加权平均,因为(等比数列求和公式)
$$
(1-\alpha)^n+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}=1
$$
$R_i$的权值即为
$$
\alpha (1-\alpha)^{n-i}
$$
权值以指数形式递减.

设$\alpha_n(a)$表示处理第$n$次选择动作$a$,选择$\alpha_n(a)=\frac{1}{n}$将会得到采样平均法,大数定律可以确保收敛到真值.

随机逼近理论中的一个结果给出了保证收敛概率为1的条件:
$$
\sum_{n=1}^\infty\alpha_n(a)=\infty \quad \text{且}\quad \sum_{n=1}^\infty\alpha^2_n(a)<\infty\
$$
第1个条件要求步长要足够大,第2个条件要求最终步长变小.显然常数步长$\alpha_n(a)=\alpha$不满足第二个条件.

### 2.6 乐观初始值

目前为止的方法都在一定程度上依赖于$Q_1(a)$的选择,这些方法是**有偏**的.

**乐观初始价值**的方法给初始值设定一个过分乐观的估计.这种乐观的估计会鼓励动作-价值方法去试探,因为无论哪种动作被选择,其收益都比估计值要小,学习器会转而尝试另一个动作.所有的动作在收敛前都被尝试过好几次.

**乐观初始价值法**在平稳问题中非常有效.

### 2.7 基于置信度上界的动作选择

**贪心**算法虽然在当前时刻看起来最好,但其他动作可能在长远上更好.

**$\epsilon$-贪心算法**会尝试选择非贪心动作,但是这种选择是盲目的,它不打会选择接近贪心或不确定性大的动作.

非贪心动作中,最好能根据他们的**潜力**来选择事实上可能是最优的动作.

一种有效的方法是根据以下公式来选择:
$$
A_t\doteq \underset{a}{\arg \max}[Q_t(a)+c\sqrt{\frac{\ln{t}}{N_t(a)}}]
$$
式中,$N_t(a)$表示动作$a$被选择的次数. $c$是一个大于0的数,控制试探的程度. 若$N_t(a)=0$,则 $a$被认为是满足最大化条件的动作.

这种方法成为基于**置信度上界**(upper confidence bound)的动作选择.参数$c$决定了置信水平.每次选 $a$时,不确定性都会减小.

### 2.8 梯度赌博机算法

我们针对每个动作考虑一个偏好函数 $H_t(a)$, $H_t(a)$越大,动作就越频繁地被选择.

偏好函数不是从"收益"的意义上提出的,一个动作对另一个动作的**相对偏好**才重要.

如果给每个动作的偏好函数加上1000,按照如下的softmax分布确定的动作概率没有任何影响.
$$
Pr\{A_t=a\}\doteq\frac{e^{H_t(a)}}{\sum^k_{b=1}e^{H_t(b)}}\doteq\pi_t(a)
$$
基于随机梯度上升的思想,提出一种自然学习方法.
$$
H_{t+1}(A_t)\doteq H_t(A_t)+\alpha(R_t-\bar R_t)(1-\pi_t(A_t))\\
H_{t+1}(a)\doteq H_t(a)-\alpha(R_t-\bar R_t)\pi_t(a), \quad a\neq A_t
$$
式中, $\alpha$是一个大于零的数,表示步长. $\bar {R}_t \in \mathbb{R}$表示时刻 $t$内所有收益的平均值.观察可得,当收益大于 $\bar {R}_t$时,未来选择动作 $A_t$的概率就会增加,反之概率就会降低.$\bar {R}_t$称为基准项.

### 2.9 关联搜索

例子:假设你遇到一个$k$臂赌博机任务,你会得到关于这个任务的编号的明显线索(不是价值),比如它的外观颜色和它的动作价值几何一一对应,动作价值集合改变,颜色也改变.那么你可以学习相关的操作**策略**,比如如果为红色,则选择1号臂;如果为绿色,则选择2号臂.

这是一个**关联搜索**的例子,既涉及试错学习去**搜索**最有动作,又将动作与情境**关联**在一起.

关联搜索任务介于$k$臂赌博机问题和完全强化学习问题之间.

### 2.10 小结

$\epsilon$-贪心方法在一小段时间内进行随机的动作选择.

UCB虽然采用确定的动作选择,却可通过每个时刻对那些具有较少样本的动作优先选择试探.

梯度赌博机不估计动作价值,而是利用偏好函数,使用softmax分布来以一种分级的、概率式的方式选择更优的动作.

在这个问题上,UCB表现得更好.



## 第三章

### 3.1"智能体-环境"交互接口

**智能体**(agent):进行学习及实施决策的机器

**环境(environment)**:智能体之外所有与其相互作用的事物

智能体选择动作,环境对这些动作做出响应,并向智能体呈现新的状态.环境也会产生一个收益,通常是特定的数值,这就是智能体在动作选择过程中想要最大化的目标.

!["智能体-环境"交互](D:\RL\notes\3.1.png)

具体地说,在每个离散时刻 $t=0,1,2,3,\cdots$,智能体和环境都发生交互.

在每个时刻 $t$,智能体观察到所在的环境**状态**的某种特征表达,$S_t\in \mathcal{S},并在此基础上选择下一个动作, $A_t\in \mathcal{A}(s)$. 下一时刻,作为其动作的结果,智能体接收到一个数值化的**收益**, $R_{t+1}\in \mathcal{R} \subset \mathbb{R}$, 并进入下一个状态 $S_{t+1}$.

MDP和智能体构成的序列或**轨迹**为:
$$
S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\cdots
$$
有限MDP:状态、动作和收益的集合 $(\mathcal{S},\mathcal{A},\mathcal{R})$只有有限个元素. 随机变量 $R_t$和 $S_t$有定义明确的离散概率分布,且只依赖于前继状态和动作.也就是说, 给定前继状态和动作的值时, $s' \in \mathcal{S}$和 $r\in \mathcal{R}$, 在$t$时刻出现的概率是
$$
p(s',r|s,a)\doteq Pr\{S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a\}
$$
函数$p$定义了MDP的**动态特性**.函数$p$为每个$s$和$a$的选择指定了一个概率分布,即
$$
\sum_{s'\in\mathcal{S}}\sum_{r\in \mathcal{R}}p(s',r|s,a)=1,对于所有s\in\mathcal{S},a\in\mathcal{A}(s)
$$
在MDP中,$S_t$和$R_t$每个可能的值出现的概率只取决于前一个状态$S_{t-1}$和前一个动作$A_{t-1}$,且与更早的状态和动作无关.

从四参数动态函数$p$中,我们可以计算出其他信息,如**状态转移概率** $p:\mathcal{S}\times\mathcal{S}\times\mathcal{A}\rightarrow[0,1]$:
$$
p(s',s|a)\doteq Pr\{S_t=s'|S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathcal{R}}p(s',r|s,a)
$$
"状态-动作"二元组的期望收益 $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$:
$$
r(s,a)\doteq\mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)
$$
"状态-动作-后继状态"三元组的期望收益 $r:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$:
$$
r(s,a,s')\doteq \mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s']=r\frac{p(s',r|s,a)}{p(s'|s,a)}
$$
