#!/usr/bin/python
# -*- coding: UTF-8 -*-
"""
@author:qzz
@file:dyna.py
@time:2022/04/08
"""

import gym
import numpy as np


def dyna_q(env: gym.Env, n: int, num_episodes: int, eps: float = 0.1, alpha: float = 0.5, gamma: float = 0.95):
    """
    Dyna-Q algorithm according to 8.2
    Args:
        env: environment generated by gym
        n: time of planning
        num_episodes: number of episodes
        eps: epsilon of epsilon-greedy
        alpha: alpha for updating Q
        gamma: discount

    Returns:
        q: the q table
        policy: policy after the algorithm
        history: history of reward

    """
    assert type(env.action_space) == gym.spaces.Discrete
    assert type(env.observation_space) == gym.spaces.Discrete
    history = [0]

    n_state, n_action = env.observation_space.n, env.action_space.n

    # initialize q table
    q = np.zeros([n_state, n_action], dtype=np.float)

    # Initialize policy to equal-probable random
    policy = np.ones([n_state, n_action], dtype=np.float) / n_action
    assert np.allclose(np.sum(policy, axis=1), 1)

    # initialize model
    model = {}

    for episode in range(num_episodes):
        state = env.reset()

        done = False
        while not done:
            # get action (b)
            action = np.random.choice(n_action, p=policy[state])
            # get r,s' (c)
            next, reward, done, info = env.step(action)
            history += [reward]

            # update q (d)
            q[state, action] += alpha * (reward + gamma * np.max(q[next]) - q[state, action])

            # update model (e)
            model[state, action] = next, reward

            # planning (f)
            transitions = list(model.keys())
            for i in range(n):
                p_state, p_action = transitions[np.random.choice(len(transitions))]
                p_next, p_reward = model[p_state, p_action]
                q[p_state, p_action] += alpha * (p_reward + gamma * np.max(q[p_next]) - q[p_state, p_action])

            # epsilon-greedy
            policy[state, :] = eps / n_action
            policy[state, np.argmax(q[state])] = 1 - eps + eps / n_action
            assert np.allclose(np.sum(policy, axis=1), 1)

            # change state (a)
            state = next
    return q, policy, history


def dyna_q_plus(env: gym.Env, n: int, num_episodes: int, eps: float = 0.1, alpha: float = 0.5, gamma: float = 0.95,
                kappa: float = 1e-4, action_only=False):
    """

    Args:
        env: environment generated by gym
        n: time of planning
        num_episodes: number of episodes
        eps: epsilon of epsilon-greedy
        alpha: alpha for updating Q
        gamma: discount
        kappa: parameter for exploring
        action_only: whether the reward kappa*sqrt(tau) will be used for updating Q

    Returns:
        q: the q table
        policy: policy after the algorithm
        history: history of reward
    """
    assert type(env.action_space) == gym.spaces.Discrete
    assert type(env.observation_space) == gym.spaces.Discrete
    history = [0]

    n_state, n_action = env.observation_space.n, env.action_space.n

    # initialize q table and tau table
    q = np.zeros([n_state, n_action], dtype=np.float)
    tau = np.zeros([n_state, n_action], dtype=np.int)

    # Initialize policy to equal-probable random
    policy = np.ones([n_state, n_action], dtype=np.float) / n_action
    assert np.allclose(np.sum(policy, axis=1), 1)

    # initialize model
    model = {}

    for episode in range(num_episodes):
        state = env.reset()

        done = False
        while not done:
            # get action
            action = np.random.choice(n_action, p=policy[state])
            # get r,s'
            next, reward, done, info = env.step(action)
            history += [reward]

            # update q
            q[state, action] += alpha * (reward + gamma * np.max(q[next]) - q[state, action])

            model.setdefault(state, {})[action] = next, reward
            tau[state, action] = 0

            # planning
            states = list(model.keys())
            for i in range(n):
                p_state = states[np.random.choice(len(states))]
                p_action = np.random.choice(n_action)
                p_next, p_reward = model[p_state].get(p_action, (p_state, 0))
                bonus = 0 if action_only else kappa * np.sqrt(tau[p_state, p_action])
                q[p_state, p_action] += alpha * (p_reward + bonus + gamma * np.max(q[p_next]) - q[p_state, p_action])

            # epsilon-greedy
            bonus = kappa * np.sqrt(tau[state]) if action_only else 0
            policy[state, :] = eps / n_action
            policy[state, np.argmax(q[state]+bonus)] = 1 - eps + eps / n_action
            assert np.allclose(np.sum(policy, axis=1), 1)

            # change state
            state = next
            tau += 1

    return q, policy, history


if __name__ == '__main__':
    pass
